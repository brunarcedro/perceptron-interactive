# Tutorial Completo: Perceptron

## üìö √çndice
1. [Introdu√ß√£o](#introdu√ß√£o)
2. [Fundamentos Te√≥ricos](#fundamentos-te√≥ricos)
3. [Implementa√ß√£o Passo a Passo](#implementa√ß√£o-passo-a-passo)
4. [Datasets e Experimentos](#datasets-e-experimentos)
5. [An√°lise de Resultados](#an√°lise-de-resultados)
6. [Limita√ß√µes e Extens√µes](#limita√ß√µes-e-extens√µes)

## Introdu√ß√£o

O Perceptron √© um dos algoritmos mais fundamentais do aprendizado de m√°quina, proposto por Frank Rosenblatt em 1957. √â um classificador bin√°rio linear que forma a base para o entendimento de redes neurais mais complexas.

### Conceitos Fundamentais:
- **Classificador Linear**: Separa classes usando um hiperplano
- **Supervisionado**: Aprende a partir de exemplos rotulados
- **Online**: Pode aprender incrementalmente
- **Converg√™ncia Garantida**: Para dados linearmente separ√°veis

## Fundamentos Te√≥ricos

### Modelo Matem√°tico

O Perceptron calcula uma soma ponderada das entradas e aplica uma fun√ß√£o de ativa√ß√£o:

```
y = f(w‚ÇÅx‚ÇÅ + w‚ÇÇx‚ÇÇ + ... + w‚Çôx‚Çô + b)
```

Onde:
- `x‚ÇÅ, x‚ÇÇ, ..., x‚Çô` s√£o as features de entrada
- `w‚ÇÅ, w‚ÇÇ, ..., w‚Çô` s√£o os pesos
- `b` √© o bias (termo de deslocamento)
- `f` √© a fun√ß√£o de ativa√ß√£o (step function)

### Fun√ß√£o de Ativa√ß√£o

A fun√ß√£o step (degrau) √© definida como:

```
f(z) = {
    1, se z ‚â• 0
    0, se z < 0
}
```

### Regra de Aprendizado

O Perceptron atualiza seus pesos usando a regra delta:

```
w_novo = w_antigo + Œ∑ √ó (y_real - y_pred) √ó x
b_novo = b_antigo + Œ∑ √ó (y_real - y_pred)
```

Onde:
- `Œ∑` √© a taxa de aprendizado (learning rate)
- `y_real` √© a classe verdadeira
- `y_pred` √© a classe predita

## Implementa√ß√£o Passo a Passo

### Passo 1: Inicializa√ß√£o

```javascript
class Perceptron {
    constructor(learningRate = 0.01, nEpochs = 100) {
        this.learningRate = learningRate;
        this.nEpochs = nEpochs;
        this.weights = null;
        this.bias = null;
    }
}
```

### Passo 2: Fun√ß√£o de Ativa√ß√£o

```javascript
activation(x) {
    return x >= 0 ? 1 : 0;
}
```

### Passo 3: Treinamento

```javascript
fit(X, y) {
    // Inicializar pesos com zeros
    this.weights = new Array(X[0].length).fill(0);
    this.bias = 0;
    
    for (let epoch = 0; epoch < this.nEpochs; epoch++) {
        let errors = 0;
        
        for (let i = 0; i < X.length; i++) {
            // Calcular sa√≠da
            let linearOutput = this.bias;
            for (let j = 0; j < X[i].length; j++) {
                linearOutput += X[i][j] * this.weights[j];
            }
            
            // Aplicar fun√ß√£o de ativa√ß√£o
            const yPred = this.activation(linearOutput);
            
            // Atualizar pesos se houver erro
            const error = y[i] - yPred;
            if (error !== 0) {
                const update = this.learningRate * error;
                this.bias += update;
                for (let j = 0; j < X[i].length; j++) {
                    this.weights[j] += update * X[i][j];
                }
                errors++;
            }
        }
        
        // Parar se convergiu
        if (errors === 0) break;
    }
}
```

### Passo 4: Predi√ß√£o

```javascript
predict(X) {
    const predictions = [];
    for (let i = 0; i < X.length; i++) {
        let sum = this.bias;
        for (let j = 0; j < X[i].length; j++) {
            sum += X[i][j] * this.weights[j];
        }
        predictions.push(this.activation(sum));
    }
    return predictions;
}
```

## Datasets e Experimentos

### Dataset 1: Blobs Sint√©ticos

**Caracter√≠sticas:**
- Linearmente separ√°vel
- Dois clusters gaussianos
- Ideal para demonstra√ß√£o

**Experimento:**
1. Gere o dataset com 200 amostras
2. Use learning rate = 0.01
3. Observe converg√™ncia em ~10-20 √©pocas
4. Acur√°cia esperada: ~100%

### Dataset 2: Iris (Setosa vs Versicolor)

**Caracter√≠sticas:**
- Dataset real de flores
- Duas classes linearmente separ√°veis
- 2 features selecionadas para visualiza√ß√£o

**Experimento:**
1. Use apenas comprimento de s√©pala e p√©tala
2. Normalize os dados
3. Compare com diferentes learning rates
4. Acur√°cia esperada: 95-100%

### Dataset 3: Moons

**Caracter√≠sticas:**
- N√ÉO linearmente separ√°vel
- Formato de duas luas entrela√ßadas
- Demonstra limita√ß√µes do Perceptron

**Experimento:**
1. Varie o n√≠vel de ru√≠do
2. Observe que nunca converge
3. Acur√°cia m√°xima: ~60%
4. A linha reta n√£o consegue separar as luas

### Dataset 4: Classifica√ß√£o com Ru√≠do

**Caracter√≠sticas:**
- Controle sobre separa√ß√£o e ru√≠do
- √ötil para testar robustez

**Experimento:**
1. Comece com separa√ß√£o = 3.0 (f√°cil)
2. Reduza gradualmente at√© 0.5
3. Observe degrada√ß√£o da performance
4. Identifique o ponto de falha

## An√°lise de Resultados

### M√©tricas de Avalia√ß√£o

1. **Acur√°cia**: Taxa de acertos geral
   ```
   Acur√°cia = (VP + VN) / (VP + VN + FP + FN)
   ```

2. **Precis√£o**: Taxa de verdadeiros positivos entre os preditos positivos
   ```
   Precis√£o = VP / (VP + FP)
   ```

3. **Recall**: Taxa de verdadeiros positivos entre os reais positivos
   ```
   Recall = VP / (VP + FN)
   ```

4. **F1-Score**: M√©dia harm√¥nica de precis√£o e recall
   ```
   F1 = 2 √ó (Precis√£o √ó Recall) / (Precis√£o + Recall)
   ```

### Interpreta√ß√£o da Fronteira de Decis√£o

Para 2D, a equa√ß√£o da fronteira √©:
```
w‚ÇÅx‚ÇÅ + w‚ÇÇx‚ÇÇ + b = 0
```

Resolvendo para x‚ÇÇ:
```
x‚ÇÇ = -(w‚ÇÅ/w‚ÇÇ)x‚ÇÅ - (b/w‚ÇÇ)
```

Isso representa uma linha reta com:
- Inclina√ß√£o: `-w‚ÇÅ/w‚ÇÇ`
- Intercepto: `-b/w‚ÇÇ`

## Limita√ß√µes e Extens√µes

### Limita√ß√µes do Perceptron

1. **Apenas Linearmente Separ√°vel**: N√£o consegue resolver XOR
2. **Classifica√ß√£o Bin√°ria**: Limitado a duas classes
3. **Sem Probabilidades**: Sa√≠da √© discreta (0 ou 1)
4. **Sens√≠vel a Outliers**: Pode ser afetado por pontos extremos

### Extens√µes e Melhorias

1. **Multi-Layer Perceptron (MLP)**
   - Adiciona camadas ocultas
   - Resolve problemas n√£o-lineares

2. **Adaline**
   - Usa fun√ß√£o de ativa√ß√£o linear durante treinamento
   - Minimiza erro quadr√°tico

3. **Perceptron com Kernel**
   - Transforma features para espa√ßo dimensional maior
   - Permite fronteiras n√£o-lineares

4. **Regulariza√ß√£o**
   - Adiciona penalidade aos pesos grandes
   - Melhora generaliza√ß√£o

### C√≥digo de Exemplo: XOR Problem

```javascript
// XOR n√£o √© linearmente separ√°vel
const X_xor = [
    [0, 0],  // Classe 0
    [1, 1],  // Classe 0
    [0, 1],  // Classe 1
    [1, 0]   // Classe 1
];
const y_xor = [0, 0, 1, 1];

// O Perceptron simples falhar√° neste problema
// Necessita de MLP ou transforma√ß√£o de features
```

## Exerc√≠cios Pr√°ticos

### Exerc√≠cio 1: Varia√ß√£o de Learning Rate
1. Use o dataset Blobs
2. Teste learning rates: 0.001, 0.01, 0.1, 1.0
3. Registre n√∫mero de √©pocas at√© converg√™ncia
4. Qual √© o trade-off?

### Exerc√≠cio 2: An√°lise de Converg√™ncia
1. Implemente early stopping
2. Registre erro por √©poca
3. Identifique padr√µes de converg√™ncia
4. Compare diferentes datasets

### Exerc√≠cio 3: Feature Engineering
1. Pegue o dataset Moons
2. Adicione features polinomiais (x‚ÇÅ¬≤, x‚ÇÇ¬≤, x‚ÇÅ√óx‚ÇÇ)
3. Retreine o Perceptron
4. Melhora a acur√°cia?

## Conclus√£o

O Perceptron √© fundamental para entender redes neurais modernas. Embora limitado a problemas linearmente separ√°veis, seus princ√≠pios formam a base para arquiteturas mais complexas. A compreens√£o profunda do Perceptron facilita o aprendizado de deep learning e outras t√©cnicas avan√ßadas.

## Refer√™ncias

1. Rosenblatt, F. (1958). "The perceptron: A probabilistic model"
2. Minsky, M. & Papert, S. (1969). "Perceptrons"
3. Bishop, C. M. (2006). "Pattern Recognition and Machine Learning"
4. Haykin, S. (2008). "Neural Networks and Learning Machines"